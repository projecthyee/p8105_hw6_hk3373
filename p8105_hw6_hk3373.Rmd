---
title: "P8105 Homework 6"
author: "Hyun Kim (hk3373)"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)

theme_set(theme_minimal() + 
  theme(legend.position = "bottom", 
      plot.title = element_text(hjust = 0.5)))

knitr::opts_chunk$set(
	fig.width = 8, 
  fig.height = 6
)
```

# Problem 1

## Download 2017 Central Park weather data
```{r download_weather_data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

## Use 5000 bootstrap samples and produce estimates for each boostrap sample
```{r produce_sample_estimates}
boot_strap_df =
  weather_df |>
  bootstrap(n = 5000) |>
  mutate(models = map(strap, \(df) lm(tmax ~ tmin, data = df)))

r_squared_df =
  boot_strap_df |>
  mutate(r_squared = map(models, broom::glance)) |> 
  select(-strap, -models) |> 
  unnest(r_squared)

log_beta_df = 
  boot_strap_df |>
  mutate(beta = map(models, broom::tidy)) |> 
  select(-strap, -models) |> 
  unnest(beta) |>
  select(.id, term, estimate) |>
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |>
  rename(intercept = "(Intercept)") |>
  mutate(log_beta = log(intercept * tmin))
```

## Plot distribution of the estimates

### Estimate of r^2
```{r plot_r_squared}
r_squared_df |>
  ggplot(aes(x = r.squared)) +
  geom_density() + 
  labs(y = "count",
       x = "Estimate of r^2",
       title = "Distribution of the Estimate of r^2")
```

The plot shows that the distribution of the estimate of r^2 is normal.

### Estimate of log(B0 * B1)
```{r plot_log_beta}
log_beta_df |>
  ggplot(aes(x = log_beta)) + 
  geom_density() + 
  labs(y = "count",
       x = "Estimate of log(B0 * B1)",
       title = "Distribution of the Estimate of log(B0 * B1)")
```

The plot shows that the distribution of the estimate of log(B0 * B1) is normal.

## Compute 95% confidence interval for the estimates

### CI of r^2 Estimate
```{r r_squared_CI}
r_squared_df |>
  summarize(
    r2_CI_lower = quantile(r.squared, 0.025),
    r2_CI_upper = quantile(r.squared, 0.975)
  ) |>
  knitr::kable(digits = 3)
```

### CI of log(B0 * B1) Estimate
```{r log_beta_CI}
log_beta_df |>
  summarize(
    log_CI_lower = quantile(log_beta, 0.025),
    log_CI_upper = quantile(log_beta, 0.975)
  ) |>
  knitr::kable(digits = 3)
```

# Problem 2

## Import and tidy Homicide dataset
```{r tidy_homicide_dataset}
homicide_df = 
  read_csv(file = "data/homicide-data.csv",
           na = c("", "Unknown")) |>
  janitor::clean_names() |>
  mutate(city_state = str_c(city, state, sep = ", "),
         resolved = as.numeric(disposition == "Closed by arrest"),
         victim_age = as.numeric(victim_age), 
         victim_race = as.factor(victim_race)) |>
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", 
                             "Kansas City, MO", "Tulsa, AL")),
         victim_race %in% c("White", "Black"))
```

## Use glm to fit logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors for Baltimore, MD
```{r baltimore_logistic_regression}
baltimore_glm =
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |>
  glm(resolved ~ victim_age + victim_sex + victim_race, 
      data = _, family = binomial())
```

## Obtain the estimate and CI of the adjusted odds ratio for solving homicides comparing male victims to female victims in Baltimore, MD
```{r baltimore_odds_ratio}
baltimore_glm |>
  broom::tidy() |>
  mutate(OR = exp(estimate),
         CI_lower = exp(estimate - 1.96 * std.error),
         CI_upper = exp(estimate + 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |>
  select(OR, CI_lower, CI_upper) |>
  knitr::kable(digits = 3)
```

## Run glm and extract adjusted odds ratio & CI for each of the cities in the dataset
```{r extract_OR_CI}
city_glm_df = 
  homicide_df |>
  nest(data = -city_state) |>
  mutate(models = map(data, \(df) glm(resolved ~ victim_age + victim_sex 
                                      + victim_race, data = df, 
                                      family = binomial())),
         results = map(models, broom::tidy))|>
  select(-data, -models) |>
  unnest(results) |>
  mutate(OR = exp(estimate),
         CI_lower = exp(estimate - 1.96 * std.error),
         CI_upper = exp(estimate + 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |>
  select(city_state, OR, CI_lower, CI_upper) 
```

## Plot that shows the estimated ORs and CIs for each city
```{r plot_OR_CI}
city_glm_df |>
  ggplot(aes(y = OR, x = reorder(city_state, OR))) + 
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(y = "Estimated Odds Ratio",
       x = "City", 
       title = "Odds Ratio for Solving Homicides Comparing Male Victims to Female Victims")
```

Based on the plot, most cities have an odds ratio that is below the value of 1, 
which suggests that the odds of solving homicides with male victims are lower
than female victims for those cities, after adjusting for the other variables 
(age and sex). 

Many cities with an odds ratio lower than 1 have a confidence interval that 
does not include the value of 1, which suggests that gender has a statistical 
significance in solving homicides for those cities, after adjusting for age 
and sex. 

There are some cities, such as Atlanta and Richmond, that have an odds ratio 
approximately equal to 1, which suggests that gender has no association in 
solving homicides for those cities. 

# Problem 3

## Load and clean data for regression analysis
```{r load_clean_birthweight}
birthweight_df = 
  read_csv(file = "data/birthweight.csv") |>
  janitor::clean_names() |>
  mutate(
    babysex = 
        case_match(babysex,
            1 ~ "male",
            2 ~ "female"
        ),
    babysex = as.factor(babysex),
    frace = 
        case_match(frace,
            1 ~ "white",
            2 ~ "black", 
            3 ~ "asian", 
            4 ~ "puerto rican", 
            8 ~ "other",
            9 ~ "unknown"),
    frace = as.factor(frace),
    mrace = 
        case_match(mrace,
            1 ~ "white",
            2 ~ "black", 
            3 ~ "asian", 
            4 ~ "puerto rican",
            8 ~ "other"),
    mrace = as.factor(mrace),
    malform = as.logical(malform)
  ) 
```

## Check for missing data
```{r check_missing_data}
sum(is.na(birthweight_df))
```

There are no missing data in the birthweight dataset. 

## Modeling process

### Stepwise selection
```{r perform_stepwise_selection}
proposed_model = 
  lm(bwt ~ ., data = birthweight_df) |>
  step(direction = "backward", trace = 0)
```

For the modeling process, stepwise selection using backward elimination was 
performed to determine and select variables to be included in the proposed 
linear regression model.

After fitting a regression model with all the variables, backward elimination 
removes the least significant predictor for each step, until the model is left 
with statistically significant predictors. 

## Predictors selected by stepwise selection
```{r display_stepwise_predictors}
proposed_model |>
  broom::tidy() |>
  knitr::kable(digits = 3)
```

Based on the result of the stepwise selection, the regression model will 
include the following variables as predictors for `bwt`: 

* `babysex`
* `bhead`
* `blength`
* `delwt`
* `fincome`
* `gaweeks`
* `mheight`
* `mrace`
* `parity`
* `ppwt`
* `smoken`

### Plot the model residuals against fitted values
```{r plot_residuals_fitted_values}
birthweight_df |>
  add_residuals(proposed_model) |>
  add_predictions(proposed_model) |>
  ggplot(aes(y = resid, x = pred)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  labs(title = "Model Residuals Against Fitted Values",
       y = "Residual", 
       x = "Fitted Value")
```

## Make comparison in terms of the cross-validated prediction error

### Train/Test split
```{r train_test_split}
bwt_cv_df = 
  crossv_mc(birthweight_df, 100) |>
  mutate(train = map(train, as_tibble),
         test = map(train, as_tibble))
```

### Fit models and obtain RMSE
```{r model_cross_validation}
bwt_cv_df =
  bwt_cv_df |>
  mutate(
    model_1 = map(train, \(df) lm(bwt ~ babysex + bhead + blength + delwt 
                                 + fincome + gaweeks + mheight + mrace 
                                 + parity + ppwt + smoken, data = df)),
    model_2 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    model_3 = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df)),
  ) |>
  mutate(
    rmse_1 = map2_dbl(model_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2 = map2_dbl(model_2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_3 = map2_dbl(model_3, test, \(mod, df) rmse(model = mod, data = df))
  )
```

* `model_1` is the proposed regression model
* `model_2` includes length at birth and gestational age as predictors
* `model_3` includes head circumference, length, sex, and all interactions 
(including the three-way interaction) between these

### Plot the cross-validated prediction error for each model 
```{r plot_cv_prediction_error}
bwt_cv_df |>
  select(starts_with("rmse")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  mutate(model = fct_inorder(model)) |>
  ggplot(aes(y = rmse, x = model, fill = model)) + 
  geom_violin() + 
  labs(title = "RMSE of the Models",
       y = "RMSE", 
       x = "Model")
```

Model 1 has the lowest RMSE, while model 2 has the highest RMSE out of the 
three models. Subsequently, model 1 (the proposed regression model) is the most 
appropriate model in predicting child's birth weight.
